<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Keep Simple</title><link href="https://zhangjoto.github.io/" rel="alternate"></link><link href="https://zhangjoto.github.io/feeds/zhangjoto.atom.xml" rel="self"></link><id>https://zhangjoto.github.io/</id><updated>2014-12-22T00:00:00+08:00</updated><entry><title>Docker使用中的问题小结</title><link href="https://zhangjoto.github.io/dockershi-yong-zhong-de-wen-ti-xiao-jie.html" rel="alternate"></link><updated>2014-12-22T00:00:00+08:00</updated><author><name>zhangjoto</name></author><id>tag:zhangjoto.github.io,2014-12-22:dockershi-yong-zhong-de-wen-ti-xiao-jie.html</id><summary type="html">&lt;div class="contents topic" id="contents"&gt;
&lt;p class="topic-title first"&gt;Contents&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id1" id="id7"&gt;环境说明&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id2" id="id8"&gt;各种问题&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id3" id="id9"&gt;部分命令的临时空间需求问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id4" id="id10"&gt;空间虚耗问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#commit" id="id11"&gt;commit性能问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id5" id="id12"&gt;小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;话说上次利用Docker建立Oracle测试环境之后，环境创建和销毁都成了很轻松的事情，我似乎从此过上了幸福快乐的日子。无奈美梦从来最易醒，在这段时间的使用过程中，理想与现实的差距却逐步暴露了出来。其中一些问题甚至直接影响到Docker的可用性，让人实在不吐不快。&lt;/p&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id7"&gt;环境说明&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;吐槽之前得说明一下我对Docker的使用方式。先看一眼Docker的服务启动文件：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;#&lt;/span&gt; cat /lib/systemd/system/docker.service
&lt;span class="go"&gt;[Unit]&lt;/span&gt;
&lt;span class="go"&gt;Description=Docker Application Container Engine&lt;/span&gt;
&lt;span class="go"&gt;Documentation=http://docs.docker.com&lt;/span&gt;
&lt;span class="go"&gt;After=network.target docker.socket&lt;/span&gt;
&lt;span class="go"&gt;Requires=docker.socket&lt;/span&gt;

&lt;span class="go"&gt;[Service]&lt;/span&gt;
&lt;span class="go"&gt;ExecStart=/usr/bin/docker -d -H fd:// --storage-opt dm.datadev=/dev/vgroot/lvdocker --storage-opt dm.metadatadev=/dev/vgroot/lvdockermeta&lt;/span&gt;
&lt;span class="go"&gt;LimitNOFILE=1048576&lt;/span&gt;
&lt;span class="go"&gt;LimitNPROC=1048576&lt;/span&gt;

&lt;span class="go"&gt;[Install]&lt;/span&gt;
&lt;span class="go"&gt;WantedBy=multi-user.target&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;可以看到我在docker启动命令行后面自己加了两个storage-opt参数，主要目的是为了让Docker使用原始物理设备（逻辑卷）做为数据和元数据的存储空间。根据红帽工程师的测试[#]，Arch下Docker默认的loop-lvm模型无论是IO性能还是资源消耗（主要是cache）都是几种模型中最差的。因此我根据测试结果采用了direct-lvm模型。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id2"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id8"&gt;各种问题&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;我是使用128G的SSD做为笔记本的本地硬盘，这显然决定了我是一个空间敏感的用户。所以就从空间的使用开始说一说。&lt;/p&gt;
&lt;div class="section" id="id3"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id9"&gt;部分命令的临时空间需求问题&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;我创建了大小为20G的逻辑卷用做Docker的存储池（这几乎是我全部的剩余空间了），另外有几十M做为Docker的元数据存储空间。想一想常见的容器一般也就是300～400M上下，个人使用这样设置应该是不会有空间问题了吧？No。&lt;/p&gt;
&lt;p&gt;我们来看一个例子。&lt;/p&gt;
&lt;p&gt;首先打开两个终端，分别把它们叫做A终端和B终端。A终端准备运行docker命令，而B终端则要先su到root并执行两条命令：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; su -
&lt;span class="gp"&gt;#&lt;/span&gt; &lt;span class="nb"&gt;cd&lt;/span&gt; /var/lib/docker
&lt;span class="gp"&gt;#&lt;/span&gt; du -sh tmp
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;接下来从你已有的镜像中挑选一个大小合适的准备导出。大小合适的意思是要导出时间足够长，保证你来得及切换终端执行其它的命令；又不至于让时间长得让人失去耐心。我自己选取的是centos6的官方镜像，记得导出命令要在A终端执行：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; docker save -o /tmp/centos.tar centos:centos6
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;现在马上切换到B终端不断的执行一条命令直到A终端的docker save命令执行完毕：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;#&lt;/span&gt; du -sh tmp
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;你可以看到tmp的占用空间大小在导出过程中会逐渐增长，最大的时候与导出镜像的大小相近，导出完成的时候又变为0。&lt;/p&gt;
&lt;p&gt;这个现象不仅在执行save命令时存在，load/build命令也是一样。程序运行中使用一些临时空间是正常的，但是这样设计至少面临几个问题：&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;在Linux的一般使用场景中，/var通常建议划分为单独的文件系统，而且使用者往往认为该文件系统下存放的主要是系统日志和软件更新包的缓存。这就决定了/var文件系统通常不会太大；&lt;/li&gt;
&lt;li&gt;/var文件空间不足会使得docker save之类的操作报错&amp;quot;no space left on device&amp;quot;，但Docker使用者判断不出到底什么空间不足，因为存储池空间富裕，而docker info的输出完全看不到临时空间的状况；&lt;/li&gt;
&lt;li&gt;空间不足导致docker save失败之后，临时文件是自动删除还是不自动删除都会有问题：如果删除，Docker使用者更难判断问题所在；如果不删除，Docker使用者（开发者）就需要寻求系统管理员的帮助，而且系统日志还可能会丢失信息；&lt;/li&gt;
&lt;li&gt;如果/var文件系统空间不足是唯一的问题，要么就得删除文件系统内的其他数据，要么就要进行文件系统扩容。前者可能会妨碍系统管理员的工作，甚至是违反某种合规性；后者的代价可能是服务器停机，毕竟不是所有文件系统都支持在线扩容，何况底层还可能不是lvm而是分区；&lt;/li&gt;
&lt;li&gt;Docker的设计是一个server可以同时多个client的请求，这意味着在大型团队里这个临时空间问题还会随着团队规模而更加放大（也许Docker开发团队认为这些都是很少有人使用的命令）；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总的来说，我个人认为临时空间放在/var/lib/docker/tmp目录下问题较多：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;临时空间与存储池空间不共用需要系统安装时做更详细的规划和计算，且错误一旦出现就需要开发者有较强的技术能力来做判断；&lt;/li&gt;
&lt;li&gt;占用（习惯上的）系统日志和缓存空间会导致系统管理员和开发者互相干涉；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;至今我还没有看到Docker有相关的最佳实践介绍，但如果有的话，我一定会建议加上一句：&lt;strong&gt;为/var/lib/docker创建单独的文件系统。&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id4"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id10"&gt;空间虚耗问题&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;所谓空间虚耗，顾名思义就是有些空间不知道被用到哪去了。还是来看一看例子。&lt;/p&gt;
&lt;p&gt;下面是我的机器目前所有的镜像：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; docker images
&lt;span class="go"&gt;REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE&lt;/span&gt;
&lt;span class="go"&gt;ora                 prepared            e50218f3bfab        2 weeks ago         309.7 MB&lt;/span&gt;
&lt;span class="go"&gt;python              3                   474f82d465a5        3 weeks ago         814.9 MB&lt;/span&gt;
&lt;span class="go"&gt;python              3.4                 474f82d465a5        3 weeks ago         814.9 MB&lt;/span&gt;
&lt;span class="go"&gt;python              3.4.2               474f82d465a5        3 weeks ago         814.9 MB&lt;/span&gt;
&lt;span class="go"&gt;python              latest              474f82d465a5        3 weeks ago         814.9 MB&lt;/span&gt;
&lt;span class="go"&gt;postgres            9                   aaab661c1e3e        4 weeks ago         213.1 MB&lt;/span&gt;
&lt;span class="go"&gt;postgres            9.3                 aaab661c1e3e        4 weeks ago         213.1 MB&lt;/span&gt;
&lt;span class="go"&gt;postgres            9.3.5               aaab661c1e3e        4 weeks ago         213.1 MB&lt;/span&gt;
&lt;span class="go"&gt;postgres            latest              aaab661c1e3e        4 weeks ago         213.1 MB&lt;/span&gt;
&lt;span class="go"&gt;debian              latest              f6fab3b798be        6 weeks ago         85.1 MB&lt;/span&gt;
&lt;span class="go"&gt;debian              wheezy              f6fab3b798be        6 weeks ago         85.1 MB&lt;/span&gt;
&lt;span class="go"&gt;debian              7                   f6fab3b798be        6 weeks ago         85.1 MB&lt;/span&gt;
&lt;span class="go"&gt;debian              7.7                 f6fab3b798be        6 weeks ago         85.1 MB&lt;/span&gt;
&lt;span class="go"&gt;centos              centos6             70441cac1ed5        6 weeks ago         215.8 MB&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;看着虽然不少，但数数IMAGE ID可以看到实际上只有ora/python/postgres/debian/centos这五个不重复的镜像。就算完全不考虑镜像间共享数据的可能，这五个镜像加起来也就是占用1.7G不到的样子。而它们在存储池里的实际使用的空间是这样的：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; docker info
&lt;span class="go"&gt;Containers: 0&lt;/span&gt;
&lt;span class="go"&gt;Images: 43&lt;/span&gt;
&lt;span class="go"&gt;Storage Driver: devicemapper&lt;/span&gt;
&lt;span class="go"&gt; Pool Name: docker-254:1-1279-pool&lt;/span&gt;
&lt;span class="go"&gt; Pool Blocksize: 65.54 kB&lt;/span&gt;
&lt;span class="go"&gt; Data file: /dev/vgroot/lvdocker&lt;/span&gt;
&lt;span class="go"&gt; Metadata file: /dev/vgroot/lvdockermeta&lt;/span&gt;
&lt;span class="go"&gt; Data Space Used: 8.291 GB&lt;/span&gt;
&lt;span class="go"&gt; Data Space Total: 17.18 GB&lt;/span&gt;
&lt;span class="go"&gt; Metadata Space Used: 6.197 MB&lt;/span&gt;
&lt;span class="go"&gt; Metadata Space Total: 16.78 MB&lt;/span&gt;
&lt;span class="go"&gt; Library Version: 1.02.92 (2014-11-28)&lt;/span&gt;
&lt;span class="go"&gt;Execution Driver: native-0.2&lt;/span&gt;
&lt;span class="go"&gt;Kernel Version: 3.18.1-1-ARCH&lt;/span&gt;
&lt;span class="go"&gt;Operating System: Arch Linux&lt;/span&gt;
&lt;span class="go"&gt;CPUs: 4&lt;/span&gt;
&lt;span class="go"&gt;Total Memory: 3.753 GiB&lt;/span&gt;
&lt;span class="go"&gt;Name: ksh-zen&lt;/span&gt;
&lt;span class="go"&gt;ID: C56B:VEMA:MS6I:ZUGG:J3MO:53U6:IU7B:4HO2:PRRB:O3O2:HZVM:EEZ4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;1.7G对8.291G，空间的有效利用率大概只有20%多一点点。注意，这是我删除了所有其它容器的结果。&lt;/p&gt;
&lt;p&gt;而创建容器的结果又是如何呢？我之前有一个安装了Oracle 11.2.0.4的软件并创建了数据库的镜像（参见我之前的&lt;a class="reference external" href="https://zhangjoto.github.io/li-yong-dockerjian-li-oracle-11gshi-yan-huan-jing.html"&gt;blog文章&lt;/a&gt;），我观察到的现象是利用这个镜像创建一个容器，启动数据库并执行几个查询，再关闭数据库和容器之后，存储池的空间占用增加了100多M。而此时容器内部的变化只是多产生了一些数据库日志文件，加起来还不到1M。&lt;/p&gt;
&lt;p&gt;其实对于这个空间利用率低的问题，从原理分析是可以有很好的解释的：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Docker的devicemapper驱动是利用lvm的snapshot的机制来实现对镜像的复用的；&lt;/li&gt;
&lt;li&gt;lvm的snapshot机制实现的是对块的变化跟踪，目前lvm的块（PE）默认值是4M；&lt;/li&gt;
&lt;li&gt;也就是说，就算只有一个文件的一个字节发生了变化，按snapshot的机制也起码要复制4M的数据；&lt;/li&gt;
&lt;li&gt;多个文件发生改变时，复制的数据量既不取决于变化的数据量，也不取决于这些文件的总数据量，而是取决于这些文件存储时落在多少个数据块上；&lt;/li&gt;
&lt;li&gt;不幸的是，一般镜像里都是大量的小文件，而且linux下的软件包通常都是usr/var/etc/lib等目录各放一部分文件，这意味着文件的布局也是零散的；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以在docker没有也不可能对文件布局进行控制的情况下，空间利用率低到20%也不是什么奇怪的事情。&lt;/p&gt;
&lt;p&gt;但是解释归解释，20%的存储效率是换谁都难以接受的，更何况继续使用下去还有可能更低。这种完全无法预测的空间需求，让系统管理员如何是好呢？&lt;/p&gt;
&lt;p&gt;多说一句，实际上我就多次因为存储池空间爆掉而被迫清理镜像。&lt;/p&gt;
&lt;p&gt;所以我很怀疑是否真有在实际使用环境采用devicemapper驱动的案例。红帽工程师虽然对各种存储驱动做了全面的性能对比测试，但也许他们开发devicemapper驱动的初衷只是为了让广大发行版用户不用编译内核就能试用Docker而已。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="commit"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id11"&gt;commit性能问题&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;这个问题简单来说，就是当容器大一点的情况下，将其提交为镜像的操作耗时很长。而且同时top观察系统的iowait也达到了40%乃至更高（别忘了我的硬盘是SSD），即便是容器相对于基础镜像改变很小的情况也是如此。&lt;/p&gt;
&lt;p&gt;我怀疑这个问题与空间虚耗存在相关性。我个人的猜测是lvm的snapshot机制与Docker的镜像分层粒度不一致，因此在commit的时候Docker需要扫描所有的数据块，将容器中改变的实际文件全部找出来才能生成新的镜像层（fs layer）。&lt;/p&gt;
&lt;p&gt;不过对于这个猜测我还没有想到什么进一步验证的办法，所以就不展开讲了。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="id5"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id12"&gt;小结&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;基于上面的种种经验教训，如果要我对打算尝试Docker的同学说两句，我会说Docker很美好，但记得两件事：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;只要保证系统不整垮，那就尽一切可能上unionfs如aufs等，而不是采用devicemapper存储驱动；&lt;/li&gt;
&lt;li&gt;单独创建一个文件系统挂载到/var/lib/docker，这会为你减少很多麻烦。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最后说一句，overlayfs并入3.18的内核主线了，我很期待。&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[1]&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://developerblog.redhat.com/2014/09/30/overview-storage-scalability-docker/"&gt;Comprehensive Overview of Storage Scalability in Docker&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</summary><category term="Docker"></category><category term="容器技术"></category></entry><entry><title>一次性能测试troubleshooting记录</title><link href="https://zhangjoto.github.io/yi-ci-xing-neng-ce-shi-troubleshootingji-lu.html" rel="alternate"></link><updated>2014-11-13T00:00:00+08:00</updated><author><name>zhangjoto</name></author><id>tag:zhangjoto.github.io,2014-11-13:yi-ci-xing-neng-ce-shi-troubleshootingji-lu.html</id><summary type="html">&lt;div class="contents topic" id="contents"&gt;
&lt;p class="topic-title first"&gt;Contents&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id1" id="id8"&gt;背景&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id2" id="id9"&gt;系统基本情况&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id3" id="id10"&gt;现场诊断&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id4" id="id11"&gt;分析外部依赖&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id5" id="id12"&gt;重现问题现象&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id6" id="id13"&gt;时间消耗分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id7" id="id14"&gt;最终结果&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id8"&gt;背景&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;前一阵子，有个朋友在某客户现场的性能测试中指标一直压不上去，所以找我去从应用和中间件角度做了一次诊断。最近消息传来，他们经过不断的排查后，最终找到了罪魁祸首，正与我判断的方向相符。这次案例的最终结果和中间的诊断过程都有一定的特点，值得说道一下。&lt;/p&gt;
&lt;p&gt;首先得介绍一下整件事情的背景和性能测试整体的环境。&lt;/p&gt;
&lt;p&gt;事情的起因是我那位朋友（为了方便，我在下面把前面提到的那位朋友就简称为L），L供职的H公司有个客户正在规划将一套OLTP系统从大型机迁移到开放平台来的方案。于是该客户挑了几支核心业务交易，找来了中外好几家公司来分头做迁移并进行性能测试，客户根据测试的结果决定跟谁合作以及最终的迁移方案。跟大型机有关的事情从来不是小单，H公司为此调来了在欧洲专做COBOL代码迁移的小组，这个小组使用工具将应用代码迁移到了Linux平台，中间件则采用了Tuxedo，客户端与Tuxedo的通信是采用阻塞方式的tpcall。&lt;/p&gt;
&lt;div class="section" id="id2"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id9"&gt;系统基本情况&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;必须要说明的一点是由于只能使用该客户指定的机器接入测试网络，唯一能将信息存入个人设备的方式就是给屏幕拍照。因此下面的描述主要是根据我的记忆整理，所有命令输入我能写出来，但输出就无法复制了。这种条件下细节与事实难免会有些出入，但我会尽力保证整体的描述的准确性。&lt;/p&gt;
&lt;p&gt;迁移后的系统部署上采用的是银行业常用的三层架构：客户端、应用服务器、数据库服务器。其软件环境大致如下：&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;caption&gt;软件环境&lt;/caption&gt;
&lt;colgroup&gt;
&lt;col width="40%" /&gt;
&lt;col width="60%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;层&lt;/th&gt;
&lt;th class="head"&gt;基础软件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;客户端&lt;/td&gt;
&lt;td&gt;Windows/LoadRunner，基于私有云&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;应用服务器&lt;/td&gt;
&lt;td&gt;RHEL 5/Tuxedo 12c，基于私有云&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;数据库服务器&lt;/td&gt;
&lt;td&gt;HP-UX/NonStop数据库&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;另外据L的描述，这套应用利用LDAP来进行配置信息的统一存储、更新和同步。这个信息以及Windows/Linux基于云构建的特殊性为问题诊断带来了一些干扰，具体细节在后面的诊断过程中会提到。&lt;/p&gt;
&lt;p&gt;而性能测试中这套系统的表现大概可以用几句话概括：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;在一次压一支交易的场景下，对每支特定的业务交易，都能找到一个确定的TPS值N；&lt;/li&gt;
&lt;li&gt;一旦性能测试的结果达到N，无论是增加应用Server数量，还是增加LoadRunner的并发客户端数量，都不能带来性能指标的提升；&lt;/li&gt;
&lt;li&gt;与此同时，应用服务器/数据库服务器的CPU消耗与测试得到的TPS值正相关，最高（TPS达到N后）维持在30~40%不变；&lt;/li&gt;
&lt;li&gt;Tuxedo队列无堵塞，客户端机器无响应慢的情况；&lt;/li&gt;
&lt;li&gt;据L的同事检查，数据库无明显的等待事件出现；&lt;/li&gt;
&lt;li&gt;这些交易都是查询交易，对数据库只有读，没有写操作；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;好了，这些就是我在到达现场以前得到的全部信息。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="id3"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id10"&gt;现场诊断&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;周日L带着我到达客户现场之后，我先帮他把Tuxedo的配置检查了一遍，没发现有什么问题，尤其是没有使用XA之类带全局锁的特性；应用服务器操作系统是RHEL 5.5，很常用的版本；各台服务器在静止状态下看起来也一切正常。随后跟L交流了一下，想看看他们团队分析这些天下来有什么想法。&lt;/p&gt;
&lt;p&gt;L告诉我，在H公司调集各路专家来看过以后，各自提出了自己的见解：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;虚拟化环境可能有些资源配置或控制策略不太对；&lt;/li&gt;
&lt;li&gt;网络大概是瓶颈；&lt;/li&gt;
&lt;li&gt;LDAP不能支撑这个规模的吞吐量，或在这个吞吐量时有性能问题；&lt;/li&gt;
&lt;li&gt;说不定是Tuxedo配置不对导致的；&lt;/li&gt;
&lt;li&gt;COBOL移植代码因为是工具做的，会不会是该系统的源代码写法比较特别，移植过来的代码就有性能问题；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但问题是谁也没有拿出进一步的证据来支持自己的论点，见解止于猜测。&lt;/p&gt;
&lt;p&gt;好吧，想法太多就等于没有想法。我决定按自己的思路从头开始分析。&lt;/p&gt;
&lt;div class="section" id="id4"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id11"&gt;分析外部依赖&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;鉴于这套应用的移植是由欧洲工程师完成，H公司的中国工程师只负责性能测试相关工作，我觉得首先分析下应用特点是个靠谱的选择。一方面我得确认目前掌握的信息是否有误，另一方面也需要根据应用特点确定下一步分析的方向和手段。&lt;/p&gt;
&lt;p&gt;先挑几个应用进程看一下打开的文件描述符：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; lsof -p &amp;lt;pid&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;输出里可以看到每个进程打开了两个tcp连接，L告诉我远端端口号为30002的是与LDAP服务的连接，另一个就是与数据库服务器的连接了。除此以外的一堆输出不是标准输出标准错误就是.so动态库文件之类，没有什么可疑。&lt;/p&gt;
&lt;p&gt;于是请L启动LoadRunner，我对进程做一下跟踪，看看它会跟哪些外部资源进行交互：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; strace -F -T -ttt -o /tmp/tt.trc -p &amp;lt;pid&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;结果发现每个应用Server都有两个线程，主线程执行Tuxedo的应用处理流程，中间会与数据库服务器发生交互；另外一个线程则专门负责与LDAP服务交互。我又请L把性能测试停一下，在应用&amp;quot;静止&amp;quot;状态下又做了一次strace，发现负责与LDAP服务交互的线程仍会定期醒来与LDAP服务通信。而且两次strace的结果中都没有发现与锁同步以及信号量相关的系统调用，也就是说至少在这个性能测试的环境下，两个线程是各自独立运行的。&lt;/p&gt;
&lt;p&gt;除此之外，这个进程与外部资源的交互也就是打开、关闭和写日志文件。日志文件的问题，只要看看iowait的值就直接排除了，我在后面就不再提。&lt;/p&gt;
&lt;p&gt;至此基本可以确定:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;这套应用除了LDAP以及数据库之外，没有其他的外部依赖；&lt;/li&gt;
&lt;li&gt;性能测试不会增大LDAP服务的压力，而且主线程的执行并不等待与LDAP服务交互的结果，所以LDAP影响性能的可能性可以排除；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以接下来只要做好对客户端、应用服务器以及数据库服务器三个点的分析，相信问题一定会水落石出。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id5"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id12"&gt;重现问题现象&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;既然排除了外部依赖的影响，我就请L为我演示一下性能测试的效果。果然，在性能测试的开始阶段，TPS值基本是随着客户端并发数线性增长，但当TPS值达到780以后，客户端并发数怎么增长都不会带来TPS值的变化。而且与此同时，LoadRunner统计的平均响应时间也从20多毫秒变为了200多毫秒（此时200并发），而且粗看起来平均响应时间的变化与客户端并发数呈倒数关系。再逐步减小客户端并发数的话，平均响应时间会在原来的响应时间水平上保持几秒，然后很快降到一个新的稳定值。&lt;/p&gt;
&lt;p&gt;从这些现象来看，很象是某个系统资源出现了瓶颈，导致并发的请求在排队，因此增大并发数就等比例的增大了响应时间，而减小并发数也要先消化完堆积的请求才能等比例的减小响应时间。而且LoadRunner显示的响应时间方差值很小，似乎排队之后得到处理的机会也是按照请求的先来后到次序分配的。&lt;/p&gt;
&lt;p&gt;L告诉我，现在看到的现象与他们测试的结果是一致的。我在测试过程中登录应用服务器上进入tmadmin敲pq检查Tuxedo队列的情况，也确实没有发现队列堵塞。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; tmadmin
&lt;span class="gp"&gt;&amp;gt;&lt;/span&gt; pq
&lt;span class="gp"&gt;&amp;gt;&lt;/span&gt; quit
&lt;span class="gp"&gt;$&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;不过在我用过所有Tuxedo 11g及以下的版本中，pq命令的输出应该类似这个样子：&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Prog Name      Queue Name  # Serve Wk Queued  # Queued  Ave. Len    Machine
---------      ------------------- ---------  --------  --------    -------
simpserv       00002.01000       1         0         0       0.0     simple
WSL            00001.00001       1         0         0       0.0     simple
BBL            123456            1         0         0       0.0     simple
&lt;/pre&gt;
&lt;p&gt;其中Queued列是当前堵塞在队列里的请求个数，&amp;quot;Ave. Len&amp;quot;列则是这个Server的平均响应时间。但在他们这套环境上，&amp;quot;Ave. Len&amp;quot;列则全部是一个减号。&lt;/p&gt;
&lt;p&gt;这让我有点困惑，不会是Tuxedo 12c里这个命令不好使了吧，难道虽然它的输出为0，但请求实际上是在排队的？想了想，我决定还是别的方向再验证一下。&lt;/p&gt;
&lt;p&gt;第一个当然是看一下操作系统的消息队列：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ipcs -q
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;看到的结果是所有的队列中消息数基本都是0，偶尔有一个队列里有一个消息，也马上就被取走了。再看一下活跃中的Tuxedo Server数量：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; tmadmin &amp;lt;&amp;lt;-! | grep -v IDLE | wc -l
&lt;span class="gp"&gt;&amp;gt;&lt;/span&gt; psr
&lt;span class="gp"&gt;&amp;gt;&lt;/span&gt; quit
&lt;span class="gp"&gt;&amp;gt;&lt;/span&gt; !
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;得到的数字也基本与LoadRunner客户端并发数相等，偶尔会差一两个。三个命令互相佐证，看来Tuxedo没有出现堵塞的情况应该是可以确定了。&lt;/p&gt;
&lt;p&gt;再考虑到LoadRunner与应用通信时是tpcall，也就是说一个请求发出去以后，必须要等到对应的应答收回来才能发起第二个请求。我结合到现在为止看到的现象做出了几个推论：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;每个请求到达Tuxedo之后，马上就有Server将请求收下来进行处理；&lt;/li&gt;
&lt;li&gt;每个LoadRunner客户端在发出请求后，该请求很快就到达了Tuxedo，Tuxedo处理完毕返回应答后，应答也很快返回了LoadRunner；&lt;/li&gt;
&lt;li&gt;在平均响应时间增长了十倍的情况下，还只能偶尔看到活动Server数与LoadRunner客户端数不相等，说明LoadRunner与Tuxedo之间的通讯耗时绝对不是主要矛盾；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;后面两个推论可能来得有点突然，我在这里解释一下：&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;LoadRunner测量得到的响应时间（T），实际上主要由三部分组成：&lt;ul&gt;
&lt;li&gt;由客户端发送请求到Tuxedo的耗时（N1）；&lt;/li&gt;
&lt;li&gt;Tuxedo处理请求耗时（S），含与数据库交互需要的时间；&lt;/li&gt;
&lt;li&gt;Tuxedo发送应答到客户端耗时（N2）三部分组成；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;除此以外还有Tuxedo通信进程以及Tuxedo Server读写消息队列的耗时，但这些操作需要的时间通常都在微秒乃至纳秒级，因此我先忽略不计；&lt;/li&gt;
&lt;li&gt;既然T = N1 + S + N2，那么在时间T内，一个客户端在时间S内有一个Tuxedo Server在处理它发起的请求，时间N1 + N2内则没有；&lt;/li&gt;
&lt;li&gt;如果我观察的次数足够多的话，(活跃的Tuxedo Server数量的均值 / LoadRunner客户端并发数)会无限趋近于 (S / T)；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;既然在客户端并发数为200的情况下，我还只是能偶尔看到活跃Server数少一两个，那S / T的值显然是非常接近1的。我现在只需要解决主要矛盾，那N1和N2的值到底是多少暂时也不用深究了。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id6"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id13"&gt;时间消耗分析&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;既然现在明确时间是消耗在业务处理上了，自然就要想办法分析时间到底消耗在什么操作上了。最简单的方式就是直接去找耗时最长的系统调用，于是我又在LoadRunner平均响应时间200毫秒的情况下对Tuxedo Server做了一段时间的strace，然后尝试找出耗时达到超出50ms的系统调用。&lt;/p&gt;
&lt;p&gt;没想到不仅是没找到超过50ms的系统调用，而且连达到20～30ms的系统调用也很少。看起来这一次跟其它性能问题的典型症状不太一样，我还是先确认一下系统调用的耗时有没有问题比较安全。先看一下系统调用的统计信息：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; strace -F -c -p &amp;lt;pid&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这个命令得到的输出会是类似这样（不要误会，我的例子实际上是回家以后strace -c ls得到的）：&lt;/p&gt;
&lt;pre class="literal-block"&gt;
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
  0.00    0.000000           0        19           read
  0.00    0.000000           0         4           write
  0.00    0.000000           0        38        31 open
  0.00    0.000000           0        10           close
  0.00    0.000000           0         8         6 stat
  0.00    0.000000           0         8           fstat
  0.00    0.000000           0        18           mmap
  0.00    0.000000           0         9           mprotect
  0.00    0.000000           0         3           munmap
  0.00    0.000000           0         4           brk
  0.00    0.000000           0         2           ioctl
  0.00    0.000000           0         1         1 access
  0.00    0.000000           0         1           execve
  0.00    0.000000           0         2           getdents
  0.00    0.000000           0         1           arch_prctl
  0.00    0.000000           0         1           openat
------ ----------- ----------- --------- --------- ----------------
100.00    0.000000                   129        38 total
&lt;/pre&gt;
&lt;p&gt;里面usecs/call列就是我关心的平均每次调用耗时。&lt;/p&gt;
&lt;p&gt;我用上面这个命令对平均响应时间为20ms和200ms的情况分别做了跟踪，得到的结果是输出中有大量的read/write调用，另外还有少量的stat/mmap等调用，而且两次跟踪的read平均耗时有好几倍的差距。&lt;/p&gt;
&lt;p&gt;既然有差距就好办了。只要有时间差距，那不是个别操作耗时太长，就是多个操作的耗时同时都变长了。再次对20ms和200ms的情况分别进行跟踪采样：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; strace -F -T -ttt -o /tmp/time.trc -p &amp;lt;pid&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;两次得到的trc文件名分别为time1.trc和time2.trc。接下来的任务就是要从两个trc文件中分别选取一次完整的交易处理过程进行对比。&lt;/p&gt;
&lt;p&gt;熟悉Tuxedo的朋友可能会知道，Tuxedo Server其实是一个死循环，静止状态时阻塞在msgrcv上，收到请求消息则开始进行业务处理，最后以msgsnd送回应答消息结束。因此我只要选取trc文件中以msgrcv开始，以msgsnd结束的一段记录就是我要的内容。&lt;/p&gt;
&lt;p&gt;对比的过程不再赘述，总之结果是两段记录中每一行的系统调用、传入参数、返回值都一模一样，但是每当对文件描述符5执行read操作时，两者之间就差了十几倍。将耗时长的那段记录里对文件描述符5的read耗时加起来一看，已经占了170ms以上。&lt;/p&gt;
&lt;p&gt;再用lsof看一下文件描述符5，就是与数据库服务器之间的tcp连接。&lt;/p&gt;
&lt;p&gt;我们知道，应用程序操作数据库时调用的底层操作实际上就是首先write将指令发送到数据库服务器，然后马上调用read等待接受操作结果。其中：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;write耗时是将报文copy到内核buffer的耗时；&lt;/li&gt;
&lt;li&gt;read实际上是报文传输到数据库服务器、数据库服务器进行处理并将应答报文回送到应用服务器三者的时间总和；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此read的耗时变长要么就是应用服务器与数据库服务器的通信不畅，要么就是数据库服务器本身处理性能出了问题。&lt;/p&gt;
&lt;p&gt;于是我问L能不能看看数据库那边是什么情况，结果L说这个NonStop整个公司也只有几个人会玩，要查只能等周一了。&lt;/p&gt;
&lt;p&gt;于是我跟L说了一下接下来的排查方向：最好是先在数据库查一下处理耗时，如果正常的话就要一级一级的去查网络通信用到的设备。然后就撤了。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="id7"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id14"&gt;最终结果&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;几天前我跟L联系的时候问起排查的结果，他说病根找到了，问题出在网络上。但不是什么网络设备的问题，而是应用服务器虚拟机环境与数据库服务器的网络通信是分配了一颗CPU来处理网卡通信。结果这颗CPU有些问题，流量一大就直接跑到100%了，因此应用服务器与数据库服务器的通信都在虚拟机以外的这颗CPU上排队，性能测试自然效果不好了。&lt;/p&gt;
&lt;p&gt;回顾整个排查过程中，我只用到了top/lsof/strace等几个很常用的工具，只是对每一步得到的结果结合业务场景对里面的数字做了定量的分析而已。这个问题虽然相对比较复杂，但是方法得当的话，真的不用花两三周的时间。&lt;/p&gt;
&lt;p&gt;其实很多时候，解决问题所需要的信息已经都摆在面前了，只看我们能不能找到合理的角度去分析解读，并找出其背后含义罢了。&lt;/p&gt;
&lt;/div&gt;
</summary><category term="troubleshooting"></category><category term="strace"></category><category term="Tuxedo"></category></entry><entry><title>利用Docker建立Oracle 11g实验环境</title><link href="https://zhangjoto.github.io/li-yong-dockerjian-li-oracle-11gshi-yan-huan-jing.html" rel="alternate"></link><updated>2014-10-20T00:00:00+08:00</updated><author><name>zhangjoto</name></author><id>tag:zhangjoto.github.io,2014-10-20:li-yong-dockerjian-li-oracle-11gshi-yan-huan-jing.html</id><summary type="html">&lt;div class="contents topic" id="contents"&gt;
&lt;p class="topic-title first"&gt;Contents&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id1" id="id24"&gt;基本信息&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id2" id="id25"&gt;宿主机调整&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id3" id="id26"&gt;空间问题解决&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#kvm" id="id27"&gt;直接从kvm虚拟机迁移&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id4" id="id28"&gt;虚拟机硬盘挂载&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id6" id="id29"&gt;文件系统导入&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#docker" id="id30"&gt;Docker环境下重新安装&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id7" id="id31"&gt;基础镜像准备&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="reference internal" href="#base-image" id="id32"&gt;获取官方Base Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#oracle" id="id33"&gt;Oracle安装准备&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id11" id="id34"&gt;图形化方式安装&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#id14" id="id35"&gt;静默安装&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#dockerfile" id="id36"&gt;Dockerfile方式静默安装&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference internal" href="#image" id="id37"&gt;环境清理并创建Image&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;最近一直在研究Docker，尝试拿它做一些小型的开发试验环境，确实是相当好用。&lt;/p&gt;
&lt;p&gt;某日突发奇想，打算将我的Oracle实验环境迁到Docker中来。虽然最终得到的镜像有点头重脚轻，不太符合Docker推荐的使用方式，但相比虚拟机好歹也节省了些开销。&lt;/p&gt;
&lt;p&gt;实验过程中各种方式折腾了好几轮，感觉都不太完美。先将过程做个记录，期望将来能找到更好的办法。&lt;/p&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id24"&gt;基本信息&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;我的笔记本本地硬盘是128G的SSD，所以实验过程中使用了外置硬盘做存储，好在是USB3.0连接。另外，基本的软件版本信息如下：&lt;/p&gt;
&lt;pre class="literal-block"&gt;
OS: Arch Linux (内核3.16.4-1)64位
Docker: 1:1.3.0-1
容器OS: CentOS 6.5
Oracle: 11.2.0.4
&lt;/pre&gt;
&lt;p&gt;为了便于区分容器内外进行的操作，我将容器内部进行操作时的命令提示符全部设为 myDocker$ 或 myDocker# ，容器外部的命令提示符则是 $ 或 #。&lt;/p&gt;
&lt;p&gt;此外，为了避免每次使用Docker命令都要sudo或者考虑权限的问题，我将当前用户（swen）加入了docker组：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;#&lt;/span&gt; gpasswd -a swen docker
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;记得正在使用的当前用户要重新登录。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id2"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id25"&gt;宿主机调整&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;我自用的发行版是Arch Linux，利用Docker安装Oracle的话还有个问题要先解决：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;几年前安装的操作系统时，只为/var划了2G空间，但Docker的所有数据默认放在/var/lib/docker目录下，文件系统空间远远不能满足需要；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此在正式开始搭建Oracle实验环境前，我现有的宿主机系统上要做一些调整。&lt;/p&gt;
&lt;div class="section" id="id3"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id26"&gt;空间问题解决&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;先查看一下当前环境的情况：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; docker info
&lt;span class="go"&gt;Containers: 0&lt;/span&gt;
&lt;span class="go"&gt;Images: 0&lt;/span&gt;
&lt;span class="go"&gt;Storage Driver: devicemapper&lt;/span&gt;
&lt;span class="go"&gt; Pool Name: docker-254:1-2988-pool&lt;/span&gt;
&lt;span class="go"&gt; Pool Blocksize: 65.54 kB&lt;/span&gt;
&lt;span class="go"&gt; Data file: /var/lib/docker/devicemapper/devicemapper/data&lt;/span&gt;
&lt;span class="go"&gt; Metadata file: /var/lib/docker/devicemapper/devicemapper/metadata&lt;/span&gt;
&lt;span class="go"&gt; Data Space Used: 305.7 MB&lt;/span&gt;
&lt;span class="go"&gt; Data Space Total: 107.4 GB&lt;/span&gt;
&lt;span class="go"&gt; Metadata Space Used: 729.1 kB&lt;/span&gt;
&lt;span class="go"&gt; Metadata Space Total: 2.147 GB&lt;/span&gt;
&lt;span class="go"&gt; Library Version: 1.02.90 (2014-09-01)&lt;/span&gt;
&lt;span class="go"&gt;Execution Driver: native-0.2&lt;/span&gt;
&lt;span class="go"&gt;Kernel Version: 3.16.4-1-ARCH&lt;/span&gt;
&lt;span class="go"&gt;Operating System: Arch Linux&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;可以看到Arch Linux上安装的Docker默认使用的devicemapper存储驱动，而数据主要是存放在如下文件中：&lt;/p&gt;
&lt;blockquote&gt;
/var/lib/docker/devicemapper/devicemapper/data&lt;/blockquote&gt;
&lt;p&gt;很显然，我们可以利用软链接解决/var文件系统空间不足的问题。虽然出于性能方面的考虑我最终选择了逻辑卷存储为长期方案，不过实验的过程中空间需求不是个小问题，我决定暂时先利用外置硬盘上的空间。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; su -
&lt;span class="gp"&gt;#&lt;/span&gt; mount /dev/vgusb/lvfiles /media/storage
&lt;span class="gp"&gt;#&lt;/span&gt; systemctl stop docker
&lt;span class="gp"&gt;#&lt;/span&gt; &lt;span class="nb"&gt;cd&lt;/span&gt; /var/lib
&lt;span class="gp"&gt;#&lt;/span&gt; rm -rf docker
&lt;span class="gp"&gt;#&lt;/span&gt; mkdir /media/storeage/docker6
&lt;span class="gp"&gt;#&lt;/span&gt; ln -s /media/storeage/docker6 docker
&lt;span class="gp"&gt;#&lt;/span&gt; systemctl start docker
&lt;span class="gp"&gt;#&lt;/span&gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;为了建立一个完全干净的Docker环境，我的例子中是选择将整个docker目录删除。要注意&lt;strong&gt;这样做会丢失全部已有的镜像、容器以及元数据。&lt;/strong&gt;如果想要仍然保留原有的所有数据，那就将docker移动到你想要的地方并创建软链接。&lt;/p&gt;
&lt;p&gt;再次查看docker的全局信息，可以看到docker已经检测到/var/lib/docker是一个软链接。并自动在/media/storage/docker6目录下执行初始化，重建了所有子目录以及文件：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; docker info
&lt;span class="go"&gt;Containers: 0&lt;/span&gt;
&lt;span class="go"&gt;Images: 0&lt;/span&gt;
&lt;span class="go"&gt;Storage Driver: devicemapper&lt;/span&gt;
&lt;span class="go"&gt; Pool Name: docker-254:8-144185831-pool&lt;/span&gt;
&lt;span class="go"&gt; Pool Blocksize: 65.54 kB&lt;/span&gt;
&lt;span class="go"&gt; Data file: /media/storage/docker6/devicemapper/devicemapper/data&lt;/span&gt;
&lt;span class="go"&gt; Metadata file: /media/storage/docker6/devicemapper/devicemapper/metadata&lt;/span&gt;
&lt;span class="go"&gt; Data Space Used: 305.7 MB&lt;/span&gt;
&lt;span class="go"&gt; Data Space Total: 107.4 GB&lt;/span&gt;
&lt;span class="go"&gt; Metadata Space Used: 356.4 kB&lt;/span&gt;
&lt;span class="go"&gt; Metadata Space Total: 2.147 GB&lt;/span&gt;
&lt;span class="go"&gt; Library Version: 1.02.90 (2014-09-01)&lt;/span&gt;
&lt;span class="go"&gt;Execution Driver: native-0.2&lt;/span&gt;
&lt;span class="go"&gt;Kernel Version: 3.16.4-1-ARCH&lt;/span&gt;
&lt;span class="go"&gt;Operating System: Arch Linux&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="kvm"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id27"&gt;直接从kvm虚拟机迁移&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;之前我一直使用的Oracle实验环境是一台kvm虚拟机，而Docker支持从已有的tar或tar.gz文件导入镜像。因此如果想快速得到一个可用的环境，从kvm虚拟机生成tar文件导入Docker显然是一个不错的方案。&lt;/p&gt;
&lt;div class="section" id="id4"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id28"&gt;虚拟机硬盘挂载&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;我的kvm虚拟机是指定一个名为/dev/vgroot/lvora的逻辑卷为本地硬盘，想生成tar文件首先要读取到其中的所有文件。而且为了保障数据一致性，在虚拟机关闭状态下读取其中的文件是最佳选择。利用loop设备配合几条基础的命令就可以做到这一点&lt;a class="footnote-reference" href="#id16" id="id5"&gt;[1]&lt;/a&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; su -
&lt;span class="gp"&gt;#&lt;/span&gt; losetup -f
&lt;span class="go"&gt;/dev/loop0&lt;/span&gt;
&lt;span class="gp"&gt;#&lt;/span&gt; losetup /dev/loop0 /dev/vgroot/lvora
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;首先找到系统中空闲的loop设备名，并将虚拟机硬盘安装为loop设备，就可以用fdisk命令查看它的分区表：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;#&lt;/span&gt; fdisk -l /dev/loop0

&lt;span class="go"&gt;Disk /dev/loop0: 8 GiB, 8589934592 bytes, 16777216 sectors&lt;/span&gt;
&lt;span class="go"&gt;Units: sectors of 1 * 512 = 512 bytes&lt;/span&gt;
&lt;span class="go"&gt;Sector size (logical/physical): 512 bytes / 512 bytes&lt;/span&gt;
&lt;span class="go"&gt;I/O size (minimum/optimal): 512 bytes / 512 bytes&lt;/span&gt;
&lt;span class="go"&gt;Disklabel type: dos&lt;/span&gt;
&lt;span class="go"&gt;Disk identifier: 0x000b3009&lt;/span&gt;

&lt;span class="go"&gt;Device       Boot  Start      End  Sectors  Size Id Type&lt;/span&gt;
&lt;span class="go"&gt;/dev/loop0p1 *      2048   165887   163840   80M 83 Linux&lt;/span&gt;
&lt;span class="go"&gt;/dev/loop0p2      165888 16777215 16611328  7.9G 8e Linux LVM&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;可以看到虚拟机硬盘分为了两个分区，/dev/loop0p1为虚拟机的/boot分区，/dev/loop0p2是一个lvm物理卷，虚拟机的根分区就在其中。&lt;/p&gt;
&lt;p&gt;遗憾的是fdisk并不会自动为每个分区创建相应的设备文件，所以你在/dev目录下是找不着loop0p1这样的文件的。所幸losetup还支持指定偏移量，把loop0的一部分再映射为另一个loop设备。&lt;/p&gt;
&lt;p&gt;因为/boot分区中的内核、引导器等在容器环境中是没有作用的，所以我直接跳过第一个分区，仅仅执行了物理卷的映射：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;#&lt;/span&gt; losetup /dev/loop1 /dev/vgroot/lvora -o 84934656  &lt;span class="c"&gt;# 165888 * 512&lt;/span&gt;
&lt;span class="gp"&gt;#&lt;/span&gt; pvscan
&lt;span class="go"&gt;PV /dev/loop1   VG vgroot   lvm2 [7.92 GiB / 0    free]&lt;/span&gt;
&lt;span class="go"&gt;PV /dev/sdb2    VG vgusb    lvm2 [305.76 GiB / 135.76 GiB free]&lt;/span&gt;
&lt;span class="go"&gt;PV /dev/sda2    VG vgroot   lvm2 [114.48 GiB / 3.48 GiB free]&lt;/span&gt;
&lt;span class="go"&gt;Total: 3 [428.16 GiB] / in use: 3 [428.16 GiB] / in no VG: 0 [0   ]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;pvscan找到的卷组中，/dev/loop1对应的卷组vgroot便是虚拟机硬盘上的根卷组。它与我的笔记本上的卷组重名（习惯问题），为了后续操作方便，要先获取到卷组的UUID，利用UUID将虚拟机中的卷组改名再mount文件系统。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;#&lt;/span&gt; vgs -v
&lt;span class="go"&gt;    DEGRADED MODE. Incomplete RAID LVs will be processed.&lt;/span&gt;
&lt;span class="go"&gt;    Finding all volume groups&lt;/span&gt;
&lt;span class="go"&gt;    Finding volume group &amp;quot;vgroot&amp;quot;&lt;/span&gt;
&lt;span class="go"&gt;    Finding volume group &amp;quot;vgusb&amp;quot;&lt;/span&gt;
&lt;span class="go"&gt;    Finding volume group &amp;quot;vgroot&amp;quot;&lt;/span&gt;
&lt;span class="go"&gt;    Archiving volume group &amp;quot;vgroot&amp;quot; metadata (seqno 4).&lt;/span&gt;
&lt;span class="go"&gt;    Archiving volume group &amp;quot;vgroot&amp;quot; metadata (seqno 22).&lt;/span&gt;
&lt;span class="go"&gt;    Creating volume group backup &amp;quot;/etc/lvm/backup/vgroot&amp;quot; (seqno 22).&lt;/span&gt;
&lt;span class="go"&gt;  VG     Attr   Ext   #PV #LV #SN VSize   VFree   VG UUID                                VProfile&lt;/span&gt;
&lt;span class="go"&gt;  vgroot wz--n- 4.00m   1   3   0   7.92g      0  WOI3ap-aggr-zh4s-cBNG-PapZ-sLpk-2hhv1v&lt;/span&gt;
&lt;span class="go"&gt;  vgroot wz--n- 4.00m   1   8   0 114.48g   3.48g MNGIRS-1jdd-vMLc-iG7u-39c2-mToI-bSPuOx&lt;/span&gt;
&lt;span class="go"&gt;  vgusb  wz--n- 4.00m   1   4   0 305.76g 135.76g gBHvoJ-0154-aMfS-cyzd-ehTj-KrTF-fdd84I&lt;/span&gt;
&lt;span class="gp"&gt;#&lt;/span&gt; vgrename WOI3ap-aggr-zh4s-cBNG-PapZ-sLpk-2hhv1v vgora
&lt;span class="go"&gt;  Volume group &amp;quot;vgroot&amp;quot; successfully renamed to &amp;quot;vgora&amp;quot;&lt;/span&gt;
&lt;span class="gp"&gt;#&lt;/span&gt; pvscan
&lt;span class="go"&gt;  PV /dev/loop1   VG vgora    lvm2 [7.92 GiB / 0    free]&lt;/span&gt;
&lt;span class="go"&gt;  PV /dev/sdb2    VG vgusb    lvm2 [305.76 GiB / 135.76 GiB free]&lt;/span&gt;
&lt;span class="go"&gt;  PV /dev/sda2    VG vgroot   lvm2 [114.48 GiB / 3.48 GiB free]&lt;/span&gt;
&lt;span class="go"&gt;  Total: 3 [428.16 GiB] / in use: 3 [428.16 GiB] / in no VG: 0 [0   ]&lt;/span&gt;
&lt;span class="gp"&gt;#&lt;/span&gt; mount /dev/vgora/lvroot /media/ora
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;至此我们在/media/ora目录下就可以看到虚拟机的根分区的所有文件了。&lt;/p&gt;
&lt;p&gt;要记住容器没有自己的内核，/boot文件系统对其是没有意义的。我的虚拟机硬盘只有/boot和/两个文件系统，因此我只需要将根文件系统mount就可以看到所有我想读取的文件。如果你的虚拟机中还有更多文件系统，那你应该根据实际需求决定哪些要mount。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id6"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id29"&gt;文件系统导入&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;接下来将虚拟机中的所有文件导入为Docker的镜像。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;#&lt;/span&gt; tar --numeric-owner -c -C /media/ora . | docker import - debian:debian7
&lt;span class="go"&gt;5b05b8d7a4645b1fc371f673f9f2c18c75769be83d4c3a3abf594f8340f60a7b&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后就可以将虚拟机的文件系统和硬盘全部卸载，让它退休了：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;#&lt;/span&gt; umount /media/ora
&lt;span class="gp"&gt;#&lt;/span&gt; vgchange -a n vgora
&lt;span class="gp"&gt;#&lt;/span&gt; losetup -d /dev/loop1
&lt;span class="gp"&gt;#&lt;/span&gt; losetup -d /dev/loop0
&lt;span class="gp"&gt;#&lt;/span&gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;因为我的Oracle数据库是使用文件系统存储，所以只要文件导入完成，我的数据库就应该能直接使用了。下面测试一下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; docker images
&lt;span class="go"&gt;REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE&lt;/span&gt;
&lt;span class="go"&gt;ora11g              latest              5b05b8d7a464        2 minutes ago       6.474 GB&lt;/span&gt;
&lt;span class="gp"&gt;$&lt;/span&gt; docker run -it ora11g /bin/bash
&lt;span class="go"&gt;myDocker# su - oracle&lt;/span&gt;
&lt;span class="go"&gt;myDocker$ sqlplus &amp;#39;/as sysdba&amp;#39;&lt;/span&gt;

&lt;span class="go"&gt;SQL*Plus: Release 11.2.0.3.0 Production on Sat Oct 25 00:57:22 2014&lt;/span&gt;

&lt;span class="go"&gt;Copyright (c) 1982, 2011, Oracle.  All rights reserved.&lt;/span&gt;


&lt;span class="go"&gt;Connected to:&lt;/span&gt;
&lt;span class="go"&gt;Oracle Database 11g Enterprise Edition Release 11.2.0.3.0 - 64bit Production&lt;/span&gt;
&lt;span class="go"&gt;With the Partitioning, OLAP, Data Mining and Real Application Testing options&lt;/span&gt;

&lt;span class="go"&gt;ORACLE instance started.&lt;/span&gt;

&lt;span class="go"&gt;Total System Global Area  304807936 bytes&lt;/span&gt;
&lt;span class="go"&gt;Fixed Size                  2227864 bytes&lt;/span&gt;
&lt;span class="go"&gt;Variable Size             163578216 bytes&lt;/span&gt;
&lt;span class="go"&gt;Database Buffers          134217728 bytes&lt;/span&gt;
&lt;span class="go"&gt;Redo Buffers                4784128 bytes&lt;/span&gt;
&lt;span class="go"&gt;Database mounted.&lt;/span&gt;
&lt;span class="go"&gt;Database opened.&lt;/span&gt;
&lt;span class="go"&gt;SQL&amp;gt; select * from v$tablespace;&lt;/span&gt;

&lt;span class="go"&gt;       TS# NAME                           INC BIG FLA ENC&lt;/span&gt;
&lt;span class="go"&gt;---------- ------------------------------ --- --- --- ---&lt;/span&gt;
&lt;span class="go"&gt;         0 SYSTEM                         YES NO  YES&lt;/span&gt;
&lt;span class="go"&gt;         1 SYSAUX                         YES NO  YES&lt;/span&gt;
&lt;span class="go"&gt;         2 UNDOTBS1                       YES NO  YES&lt;/span&gt;
&lt;span class="go"&gt;         4 USERS                          YES NO  YES&lt;/span&gt;
&lt;span class="go"&gt;         3 TEMP                           NO  NO  YES&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里可以看到从虚拟机迁移来的Oracle版本是11.2.0.3，不过这只是这套环境安装太早的原因，对整个迁移过程没有什么影响。&lt;/p&gt;
&lt;p&gt;至此可以确认Oracle环境已经迁移成功。使用这个方法，必然会把原环境的一些临时性文件带入镜像，造成空间浪费。如果对镜像的空间使用和整洁规范要求很高的话，你要在生成tar包之前手工进行清理。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="docker"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id30"&gt;Docker环境下重新安装&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;穷举Oracle官方和Docker提供的方法，Docker环境下搭建Oracle数据库实验环境的方法不外乎三种：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;容器内命令行执行图形安装&lt;/li&gt;
&lt;li&gt;容器内命令行执行静默安装&lt;/li&gt;
&lt;li&gt;编辑Dockerfile进行静默安装&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;无论选择哪一种，操作系统准备、缺失软件包安装、核心参数修改、用户创建等工作总是要做的。我打算将这些工作做好之后单独创建一个镜像，避免重复劳动。&lt;/p&gt;
&lt;div class="section" id="id7"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id31"&gt;基础镜像准备&lt;/a&gt;&lt;/h3&gt;
&lt;div class="section" id="base-image"&gt;
&lt;h4&gt;&lt;a class="toc-backref" href="#id32"&gt;获取官方Base Image&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Docker官方提供了多种操作系统基础镜像，我们可以直接通过docker命令下载到本地。不过由于众所周知的网络原因，国内用户成功执行这步操作的难度颇高。有几种办法可以破：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;为docker后台进程上HTTP代理&lt;a class="footnote-reference" href="#id17" id="id8"&gt;[2]&lt;/a&gt;；&lt;/li&gt;
&lt;li&gt;使用国内镜像代替&lt;a class="footnote-reference" href="#id18" id="id9"&gt;[3]&lt;/a&gt;；&lt;/li&gt;
&lt;li&gt;利用openvz的模板&lt;a class="footnote-reference" href="#id19" id="id10"&gt;[4]&lt;/a&gt;导入；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;利用openvz模板的方法我测试过是可行的，不过体积比Docker官方镜像大了不小，所以一般还是建议用代理或者国内镜像好了，有特殊需求的朋友可以考虑openvz的模板。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; docker pull centos:centos6
&lt;span class="go"&gt;centos:centos6: The image you are pulling has been verified&lt;/span&gt;
&lt;span class="go"&gt;5b12ef8fd570: Pulling fs layer&lt;/span&gt;
&lt;span class="go"&gt;5b12ef8fd570: Download complete&lt;/span&gt;
&lt;span class="go"&gt;68edf809afe7: Download complete&lt;/span&gt;
&lt;span class="go"&gt;511136ea3c5a: Download complete&lt;/span&gt;
&lt;span class="go"&gt;Status: Downloaded newer image for centos:centos6&lt;/span&gt;
&lt;span class="gp"&gt;$&lt;/span&gt; docker images
&lt;span class="go"&gt;REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE&lt;/span&gt;
&lt;span class="go"&gt;centos              centos6             68edf809afe7        3 weeks ago         212.7 MB&lt;/span&gt;
&lt;span class="gp"&gt;$&lt;/span&gt;docker info
&lt;span class="go"&gt;Containers: 0&lt;/span&gt;
&lt;span class="go"&gt;Images: 3&lt;/span&gt;
&lt;span class="go"&gt;Storage Driver: devicemapper&lt;/span&gt;
&lt;span class="go"&gt; Pool Name: docker-254:8-144185831-pool&lt;/span&gt;
&lt;span class="go"&gt; Pool Blocksize: 65.54 kB&lt;/span&gt;
&lt;span class="go"&gt; Data file: /media/storage/docker6/devicemapper/devicemapper/data&lt;/span&gt;
&lt;span class="go"&gt; Metadata file: /media/storage/docker6/devicemapper/devicemapper/metadata&lt;/span&gt;
&lt;span class="go"&gt; Data Space Used: 577.7 MB&lt;/span&gt;
&lt;span class="go"&gt; Data Space Total: 107.4 GB&lt;/span&gt;
&lt;span class="go"&gt; Metadata Space Used: 942.1 kB&lt;/span&gt;
&lt;span class="go"&gt; Metadata Space Total: 2.147 GB&lt;/span&gt;
&lt;span class="go"&gt; Library Version: 1.02.90 (2014-09-01)&lt;/span&gt;
&lt;span class="go"&gt;Execution Driver: native-0.2&lt;/span&gt;
&lt;span class="go"&gt;Kernel Version: 3.16.4-1-ARCH&lt;/span&gt;
&lt;span class="go"&gt;Operating System: Arch Linux&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;因为现在Docker官方的centos最新镜像已经升级到了7.0，所以下载的时候必须指定tag为centos6。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="oracle"&gt;
&lt;h4&gt;&lt;a class="toc-backref" href="#id33"&gt;Oracle安装准备&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;首先在Base Image的基础上安装缺失的软件包：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; docker run -it centos:centos6 /bin/bash
&lt;span class="go"&gt;myDocker# yum install -y compat-libstdc++-33 elfutils-libelf-devel elfutils-libelf-devel-static ksh libaio libaio-devel libstdc++-devel make numactl-devel sysstat gcc-c++&lt;/span&gt;
&lt;span class="go"&gt;myDocker# yum install -y libXext&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;第一行命令中安装的make/gcc-c++/libaio等一系列软件包是Oracle的官方文档中列出的软件包需求，而缺失libXext则是在实验的过程中发现的问题。如果不安装libXext的话，Oracle安装程序会无法启动。&lt;/p&gt;
&lt;p&gt;接下来是调整核心参数以及用户ulimit相关的限制。按照Oracle官方文档，Linux环境下安装Oracle时核心参数建议值如下：&lt;/p&gt;
&lt;pre class="literal-block"&gt;
fs.aio-max-nr = 1048576
fs.file-max = 6815744
kernel.shmall = 2097152
kernel.shmmax = 4294967295
kernel.shmmni = 4096
kernel.sem = 250 32000 100 128
net.ipv4.ip_local_port_range = 9000 65500
net.core.rmem_default = 262144
net.core.rmem_max = 4194304
net.core.wmem_default = 262144
net.core.wmem_max = 1048576
&lt;/pre&gt;
&lt;p&gt;但实际上，Base Image中一些核心参数（目前主要是共享内存相关）默认值就已经比Oracle的建议值还要大了。经过比对，我去掉了两个shm相关的参数：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="go"&gt;myDocker# vi /etc/sysctl.conf             # 在文件最后加入以下内容并保存&lt;/span&gt;
&lt;span class="go"&gt;fs.aio-max-nr = 1048576&lt;/span&gt;
&lt;span class="go"&gt;fs.file-max = 6815744&lt;/span&gt;
&lt;span class="go"&gt;kernel.shmmni = 4096&lt;/span&gt;
&lt;span class="go"&gt;kernel.sem = 250 32000 100 128&lt;/span&gt;
&lt;span class="go"&gt;net.ipv4.ip_local_port_range = 9000 65500&lt;/span&gt;
&lt;span class="go"&gt;net.core.rmem_default = 262144&lt;/span&gt;
&lt;span class="go"&gt;net.core.rmem_max = 4194304&lt;/span&gt;
&lt;span class="go"&gt;net.core.wmem_default = 262144&lt;/span&gt;
&lt;span class="go"&gt;net.core.wmem_max = 1048576&lt;/span&gt;

&lt;span class="go"&gt;myDocker# vi /etc/security/limits.conf    # 在文件最后加入以下内容并保存&lt;/span&gt;
&lt;span class="go"&gt;oracle              soft    nproc   2047&lt;/span&gt;
&lt;span class="go"&gt;oracle              hard    nproc   16384&lt;/span&gt;
&lt;span class="go"&gt;oracle              soft    nofile  1024&lt;/span&gt;
&lt;span class="go"&gt;oracle              hard    nofile  65536&lt;/span&gt;
&lt;span class="go"&gt;oracle              soft    stack   10240&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这些调整完成以后就可以创建数据库相关的操作系统用户和组了：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="go"&gt;myDocker# groupadd -g 601 dba&lt;/span&gt;
&lt;span class="go"&gt;myDocker# groupadd -g 602 oinstall&lt;/span&gt;

&lt;span class="go"&gt;myDocker# useradd -u 601 -m -g oinstall -G dba oracle&lt;/span&gt;
&lt;span class="go"&gt;myDocker# echo &amp;quot;oracle:oracle&amp;quot; | chpasswd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;下面我要做一件官方文档上不可能提到的事情，首先用编辑器打开容器中的fstab：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="go"&gt;myDocker# vi /etc/fstab&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;可以看到如下所示的内容：&lt;/p&gt;
&lt;pre class="literal-block"&gt;
LABEL=_/   /        ext4      defaults         0 0
devpts     /dev/pts  devpts  gid=5,mode=620   0 0
tmpfs      /dev/shm  tmpfs   defaults         0 0
proc       /proc     proc    defaults         0 0
sysfs      /sys      sysfs   defaults         0 0
&lt;/pre&gt;
&lt;p&gt;现在我们找到/dev/shm所在的那一行，在defaults后指定其大小为1G。修改后这一行的内容应该是这样的：&lt;/p&gt;
&lt;pre class="literal-block"&gt;
tmpfs      /dev/shm  tmpfs   defaults,size=1g         0 0
&lt;/pre&gt;
&lt;p&gt;记得将修改后的文件保存。要做这个修改的原因是Oracle软件产品完毕后使用dbca建库时，建库脚本会利用/dev/shm创建共享内存对象，但Docker为其指定的默认大小（64M）不足以满足需求。所以我在创建数据库前要临时扩展其大小。&lt;/p&gt;
&lt;p&gt;如果不扩展/dev/shm大小的话，建库程序dbca会在执行到76%左右的时候报错挂起。如果你去查看创建数据库的日志（$ORACLE_BASE/cfgtoollogs/dbca/$ORACLE_SID/postDBCreation.log），应该会看到明确的提示。&lt;/p&gt;
&lt;p&gt;不过神奇的是这时候Oracle会在$HOME目录下生成一个名字类似&amp;quot;catbundle_PSU_SAND_GENERATE*.log&amp;quot;的日志，里面告诉你&amp;quot;Error reading bundledata_PSU.xml - patch NOT installed&amp;quot;。不要被它误导，&lt;strong&gt;一定要去看一眼数据库创建的日志文件。&lt;/strong&gt;那里面会是这样写：&lt;/p&gt;
&lt;pre class="literal-block"&gt;
BEGIN
*
ERROR at line 1:
ORA-29516: Aurora assertion failure: Assertion failure at joez.c:3422
Bulk load of method java/lang/Object.&amp;lt;init&amp;gt; failed; insufficient shm-object space
ORA-06512: at line 3


          IF CatbundleCreateDir(:catbundleLogDir) = 0 THEN
                       *
ERROR at line 71:
ORA-06550: line 71, column 14:
PLS-00201: identifier 'CATBUNDLECREATEDIR' must be declared
ORA-06550: line 71, column 11:
PL/SQL: Statement ignored
&lt;/pre&gt;
&lt;p&gt;这个问题经我测试在Oracle for linux 64位的11.2.0.3/11.2.0.4版本都存在，11.2.0.2未测试，11.2.0.1版本倒是不受影响。所以你要根据Oracle软件版本自行决定。&lt;/p&gt;
&lt;p&gt;最后我再偷个懒，提前把oracle用户建库时候要用的环境变量也全部设置好，工作量能省一点是一点：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="go"&gt;myDocker# su - oracle&lt;/span&gt;
&lt;span class="go"&gt;myDocker$ vi .bash_profile                # 在文件最后加入以下行并保存&lt;/span&gt;
&lt;span class="go"&gt;ORACLE_SID=sand&lt;/span&gt;
&lt;span class="go"&gt;ORACLE_BASE=$HOME/server&lt;/span&gt;
&lt;span class="go"&gt;ORACLE_HOME=$ORACLE_BASE/product/11.2/db&lt;/span&gt;
&lt;span class="gp"&gt;PATH=$ORACLE_HOME/bin:$PATH:$&lt;/span&gt;HOME/tools/sbin
&lt;span class="go"&gt;LD_LIBRARY_PATH=$ORACLE_HOME/lib:$LD_LIBRARY_PATH&lt;/span&gt;
&lt;span class="go"&gt;export ORACLE_SID ORACLE_BASE ORACLE_HOME PATH LD_LIBRARY_PATH&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;环境变量中的软件安装路径和数据库名我都按照自己的喜好修改过，你可以按自己的习惯自行调整，也可以遵循Oracle提供的默认设置。&lt;/p&gt;
&lt;p&gt;至此基础镜像的准备已经全部就绪，现在把这个容器清理一下，再提交为镜像。因为现在我还只有一个容器，所以命令行中Container ID只需输入头几位，让Docker知道我指的是哪一个容器就行了：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="go"&gt;myDocker$ exit&lt;/span&gt;
&lt;span class="go"&gt;myDocker# yum clean all&lt;/span&gt;
&lt;span class="go"&gt;myDocker# exit&lt;/span&gt;
&lt;span class="gp"&gt;$&lt;/span&gt; docker ps -a
&lt;span class="go"&gt;CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES&lt;/span&gt;
&lt;span class="go"&gt;0042412b5d55        centos:centos6      &amp;quot;/bin/bash&amp;quot;         9 minutes ago       Exited (0) 3 seconds ago                       jovial_kowalevski&lt;/span&gt;
&lt;span class="gp"&gt;$&lt;/span&gt; docker commit -m &lt;span class="s1"&gt;&amp;#39;prepared for oracle install&amp;#39;&lt;/span&gt; 0042 ora11g:prepared
&lt;span class="go"&gt;0042412b5d55348e8868349cf8435123614bc0926674e22dc7382e477d048586&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;再检查一下提交是否成功：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; docker images
&lt;span class="go"&gt;REPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE&lt;/span&gt;
&lt;span class="go"&gt;ora11g                          prepared            aa5e34846c4e        20 seconds ago      368.6 MB&lt;/span&gt;
&lt;span class="go"&gt;centos                          centos6             68edf809afe7        3 weeks ago         212.7 MB&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;其实还有更简单一点的方式创建基础镜像，就是利用docker build。现友情赠送创建基础镜像的Dockerfile：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;FROM centos:centos6
MAINTAINER zhangjoto@gmail.com
RUN yum install -y compat-libstdc++-33 \
    elfutils-libelf-devel \
    elfutils-libelf-devel-static \
    ksh \
    libaio \
    libaio-devel \
    libstdc++-devel \
    make \
    numactl-devel \
    sysstat \
    gcc-c++ \
    &amp;amp;&amp;amp; yum install -y libXext \
    &amp;amp;&amp;amp; yum clean all
RUN groupadd -g 601 dba \
    &amp;amp;&amp;amp; groupadd -g 602 oinstall \
    &amp;amp;&amp;amp; useradd -u 601 -g 602 -G 601 -m oracle \
    &amp;amp;&amp;amp; chmod 750 /home/oracle \
    &amp;amp;&amp;amp; echo &amp;quot;oracle:oracle&amp;quot; | chpasswd
RUN echo    &amp;quot;fs.aio-max-nr = 1048576&amp;quot; &amp;gt;&amp;gt;/etc/sysctl.conf \
    &amp;amp;&amp;amp; echo &amp;quot;fs.file-max = 6815744&amp;quot; &amp;gt;&amp;gt;/etc/sysctl.conf \
    &amp;amp;&amp;amp; echo &amp;quot;kernel.shmmni = 4096&amp;quot; &amp;gt;&amp;gt;/etc/sysctl.conf \
    &amp;amp;&amp;amp; echo &amp;quot;kernel.sem = 250 32000 100 128&amp;quot; &amp;gt;&amp;gt;/etc/sysctl.conf \
    &amp;amp;&amp;amp; echo &amp;quot;net.ipv4.ip_local_port_range = 9000 65500&amp;quot; &amp;gt;&amp;gt;/etc/sysctl.conf \
    &amp;amp;&amp;amp; echo &amp;quot;net.core.rmem_default = 262144&amp;quot; &amp;gt;&amp;gt;/etc/sysctl.conf \
    &amp;amp;&amp;amp; echo &amp;quot;net.core.rmem_max = 4194304&amp;quot; &amp;gt;&amp;gt;/etc/sysctl.conf \
    &amp;amp;&amp;amp; echo &amp;quot;net.core.wmem_default = 262144&amp;quot; &amp;gt;&amp;gt;/etc/sysctl.conf \
    &amp;amp;&amp;amp; echo &amp;quot;net.core.wmem_max = 1048576&amp;quot; &amp;gt;&amp;gt;/etc/sysctl.conf
RUN echo    &amp;quot;oracle              soft    nproc   2047&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf \
    &amp;amp;&amp;amp; echo &amp;quot;oracle              hard    nproc   16384&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf \
    &amp;amp;&amp;amp; echo &amp;quot;oracle              soft    nofile  1024&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf \
    &amp;amp;&amp;amp; echo &amp;quot;oracle              hard    nofile  65536&amp;quot; &amp;gt;&amp;gt;/etc/security/limits.conf \
    &amp;amp;&amp;amp; echo &amp;quot;oracle              soft    stack   10240&amp;quot; &amp;gt;&amp;gt; /etc/security/limits.conf
RUN echo    &amp;#39;ORACLE_SID=sand&amp;#39; &amp;gt;&amp;gt;/home/oracle/.bash_profile \
    &amp;amp;&amp;amp; echo &amp;#39;ORACLE_BASE=$HOME/server&amp;#39; &amp;gt;&amp;gt;/home/oracle/.bash_profile \
    &amp;amp;&amp;amp; echo &amp;#39;ORACLE_HOME=$ORACLE_BASE/product/11.2/db&amp;#39; &amp;gt;&amp;gt;/home/oracle/.bash_profile \
    &amp;amp;&amp;amp; echo &amp;#39;PATH=$ORACLE_HOME/bin:$PATH&amp;#39; &amp;gt;&amp;gt;/home/oracle/.bash_profile \
    &amp;amp;&amp;amp; echo &amp;#39;LD_LIBRARY_PATH=$ORACLE_HOME/lib:$LD_LIBRARY_PATH&amp;#39; &amp;gt;&amp;gt;/home/oracle/.bash_profile \
    &amp;amp;&amp;amp; echo &amp;#39;export ORACLE_SID ORACLE_BASE ORACLE_HOME PATH LD_LIBRARY_PATH&amp;#39; &amp;gt;&amp;gt;/home/oracle/.bash_profile
RUN sed -i &amp;#39;/\/dev\/shm/s/defaults/defaults,size=1g/&amp;#39; /etc/fstab
CMD /bin/bash
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;因为Dockerfile对多行字符串的支持不太好，所以我用了很多个echo一行一行的向文件追加内容。如果你知道更好的方式（除了预先编辑文件COPY到容器内之外），还请记得留言告诉我。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="id11"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id34"&gt;图形化方式安装&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;容器是没有自己的图形界面的，想实现图形化安装就得将容器内部的图形程序转发到容器外部来显示。&lt;/p&gt;
&lt;p&gt;通常实现这个目的最简单的方式利用sshd的X11转发功能，但Docker官方并不建议在容器中运行sshd，&lt;a class="footnote-reference" href="#id21" id="id12"&gt;[5]&lt;/a&gt;我也不希望多装软件包。实际上如果不考虑安全性的问题，完全可以通过将X11的socket文件映射到容器内部达到目的。国外已经有人用这种方式运行容器内的Firefox和其它程序&lt;a class="footnote-reference" href="#id22" id="id13"&gt;[6]&lt;/a&gt;。不过随着Docker新版本安全性的增强，原文的办法已不再完全适用，我需要做一点小小的调整（参见原文下面的讨论）。&lt;/p&gt;
&lt;p&gt;首先从准备好的基础镜像创建准备用来安装Oracle数据库的容器，为了方便，我要将它命名为ora。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; docker run -it --name ora -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v &lt;span class="nv"&gt;$HOME&lt;/span&gt;/.Xauthority:/root/.Xauthority -v &lt;span class="nv"&gt;$HOME&lt;/span&gt;/ora:/mnt --net&lt;span class="o"&gt;=&lt;/span&gt;host --privileged ora11g:prepared /bin/bash
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;我先解释一下这行命令中各个参数的含义：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;--name 指定容器的名字为ora；&lt;/li&gt;
&lt;li&gt;-e 告诉Docker要将环境变量DISPLAY的值传入容器中；&lt;/li&gt;
&lt;li&gt;-v 将Host的文件映射到容器内部；&lt;ul&gt;
&lt;li&gt;/tmp/.X11-unix：X11协议用到的socket文件就在这个目录；&lt;/li&gt;
&lt;li&gt;$HOME/.Xauthority：使用Host上当前用户的该文件才能通过X11的权限检查；&lt;/li&gt;
&lt;li&gt;$HOME/ora：我的Oracle软件安装介质解压放在这个目录下；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;--net=host 告诉Docker容器直接使用Host的网络协议栈。这个选项是有安全风险的，我仅在需要运行容器中的图形程序时使用；&lt;/li&gt;
&lt;li&gt;--privileged 为容器申请特权。要在容器内改变/dev/shm的大小及设置核心参数都必须指定这个参数；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因为容器没有自己的init进程，因此Docker用户自定义的初始化任务都只能手工执行。比如根据fstab挂载文件系统和设置核心参数：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="go"&gt;myDocker# mount -o remount /dev/shm&lt;/span&gt;
&lt;span class="go"&gt;myDocker# df -h&lt;/span&gt;
&lt;span class="go"&gt;Filesystem            Size  Used Avail Use% Mounted on&lt;/span&gt;
&lt;span class="go"&gt;rootfs                9.8G  388M  8.9G   5% /&lt;/span&gt;
&lt;span class="go"&gt;/dev/mapper/docker-254:8-144185831-375f693461a71e033bb7c0929713ac81793f25b76d509b28652c45de7f8a0977&lt;/span&gt;
&lt;span class="go"&gt;                      9.8G  388M  8.9G   5% /&lt;/span&gt;
&lt;span class="go"&gt;tmpfs                 1.9G     0  1.9G   0% /dev&lt;/span&gt;
&lt;span class="go"&gt;shm                   1.0G     0  1.0G   0% /dev/shm&lt;/span&gt;
&lt;span class="go"&gt;/dev/mapper/vgusb-lvfiles&lt;/span&gt;
&lt;span class="go"&gt;                       50G   21G   30G  41% /etc/resolv.conf&lt;/span&gt;
&lt;span class="go"&gt;/dev/mapper/vgusb-lvfiles&lt;/span&gt;
&lt;span class="go"&gt;                       50G   21G   30G  41% /etc/hostname&lt;/span&gt;
&lt;span class="go"&gt;/dev/mapper/vgusb-lvfiles&lt;/span&gt;
&lt;span class="go"&gt;                       50G   21G   30G  41% /etc/hosts&lt;/span&gt;
&lt;span class="go"&gt;/dev/mapper/vgroot-lvhome&lt;/span&gt;
&lt;span class="go"&gt;                       80G   78G  2.3G  98% /mnt&lt;/span&gt;
&lt;span class="go"&gt;/dev/mapper/vgroot-lvhome&lt;/span&gt;
&lt;span class="go"&gt;                       80G   78G  2.3G  98% /root/.Xauthority&lt;/span&gt;
&lt;span class="go"&gt;tmpfs                 1.9G  212K  1.9G   1% /tmp/.X11-unix&lt;/span&gt;

&lt;span class="go"&gt;myDocker# sysctl -p&lt;/span&gt;
&lt;span class="go"&gt;net.ipv4.ip_forward = 0&lt;/span&gt;
&lt;span class="go"&gt;net.ipv4.conf.default.rp_filter = 1&lt;/span&gt;
&lt;span class="go"&gt;net.ipv4.conf.default.accept_source_route = 0&lt;/span&gt;
&lt;span class="go"&gt;kernel.sysrq = 0&lt;/span&gt;
&lt;span class="go"&gt;kernel.core_uses_pid = 1&lt;/span&gt;
&lt;span class="go"&gt;error: &amp;quot;net.ipv4.tcp_syncookies&amp;quot; is an unknown key&lt;/span&gt;
&lt;span class="go"&gt;error: &amp;quot;net.bridge.bridge-nf-call-ip6tables&amp;quot; is an unknown key&lt;/span&gt;
&lt;span class="go"&gt;error: &amp;quot;net.bridge.bridge-nf-call-iptables&amp;quot; is an unknown key&lt;/span&gt;
&lt;span class="go"&gt;error: &amp;quot;net.bridge.bridge-nf-call-arptables&amp;quot; is an unknown key&lt;/span&gt;
&lt;span class="go"&gt;kernel.msgmnb = 65536&lt;/span&gt;
&lt;span class="go"&gt;kernel.msgmax = 65536&lt;/span&gt;
&lt;span class="go"&gt;kernel.shmmax = 68719476736&lt;/span&gt;
&lt;span class="go"&gt;kernel.shmall = 4294967296&lt;/span&gt;
&lt;span class="go"&gt;fs.aio-max-nr = 1048576&lt;/span&gt;
&lt;span class="go"&gt;fs.file-max = 6815744&lt;/span&gt;
&lt;span class="go"&gt;kernel.shmmni = 4096&lt;/span&gt;
&lt;span class="go"&gt;kernel.sem = 250 32000 100 128&lt;/span&gt;
&lt;span class="go"&gt;net.ipv4.ip_local_port_range = 9000 65500&lt;/span&gt;
&lt;span class="go"&gt;error: &amp;quot;net.core.rmem_default&amp;quot; is an unknown key&lt;/span&gt;
&lt;span class="go"&gt;error: &amp;quot;net.core.rmem_max&amp;quot; is an unknown key&lt;/span&gt;
&lt;span class="go"&gt;error: &amp;quot;net.core.wmem_default&amp;quot; is an unknown key&lt;/span&gt;
&lt;span class="go"&gt;error: &amp;quot;net.core.wmem_max&amp;quot; is an unknown key&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;可以看到对/dev/shm的修改生效了，而设置核心参数有很多error。不过不用担心，报错的这些参数只有在繁忙的数据库环境下才真的有必要调整。至于个人使用的实验环境，我们直接跳过不去管它就好了。&lt;/p&gt;
&lt;p&gt;接下来为了启用图形化界面还要做点小小的处理：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="go"&gt;myDocker# cp ~/.Xauthority ~oracle/&lt;/span&gt;
&lt;span class="go"&gt;myDocker# chown oracle:oinstall ~oracle/.Xauthority&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;因为这个.Xauthority文件是从Host映射进来的，我不想让容器对外部环境造成干扰，因此为oracle用户单独复制了一份。如果你不想这么做，那就将容器内的oracle用户的uid和gid设为与Host内用户完全一样的值，并在前面启动容器的命令行里将该文件映射为/home/oracle/.Xauthority即可。&lt;/p&gt;
&lt;p&gt;现在设置好Oracle安装介质所在目录的权限就可以开始运行安装程序：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="go"&gt;myDocker# chown -R oracle:oinstall /mnt&lt;/span&gt;
&lt;span class="go"&gt;myDocker# su - oracle&lt;/span&gt;
&lt;span class="go"&gt;myDocker$ cd /mnt/database&lt;/span&gt;
&lt;span class="go"&gt;myDocker$ ./runInstaller&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这时你就应该能看到熟悉的安装界面了。&lt;/p&gt;
&lt;p&gt;至于剩下的安装过程、建库和执行指定的脚本以及安装后的验证检查等动作都是标准化流程，网上图文并茂的教程一搜就是一大把，我就不再赘述了。唯一需要提到的就是系统需求检查的时候会找不到pdksh包，实际上centos6里它已经改名为ksh，直接忽略警告即可。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id14"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id35"&gt;静默安装&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;静默安装的过程与图形化安装大同小异，除了准备响应文件、建库模板以外只有几个地方不太一样：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;启动容器的命令行更简略，由于是静默安装，所以图形转发相关的参数全都可以去掉：&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; docker run -it --name ora -v &lt;span class="nv"&gt;$HOME&lt;/span&gt;/ora:/mnt --privileged ora11g:prepared /bin/bash
&lt;/pre&gt;&lt;/div&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;运行安装程序的参数要为静默方式指定响应文件并忽略系统需求检查。响应文件在图形化安装时录制或使用安装介质response目录下提供的模板修改均可。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="go"&gt;myDocker$ ./runInstaller -silent -responseFile /mnt/db.rsp -ignorePrereq&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;运行dbca程序也要指定大量参数：&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="go"&gt;myDocker$ dbca -silent -createDatabase -templateName /mnt/sand.dbc \&lt;/span&gt;
&lt;span class="go"&gt;    -gdbname sand -sid sand -sysPassword sys -systemPassword sys \&lt;/span&gt;
&lt;span class="go"&gt;    -characterSet ZHS16GBK -nationalCharacterSet AL16UTF16&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;数据库模板可以从下面的目录获取官方样板来修改：&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ORACLE_HOME/assistants/dbca/templates
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="dockerfile"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id36"&gt;Dockerfile方式静默安装&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;废话不多说，先贴上Dockerfile的内容。其中runInstaller多了一个参数&amp;quot;-waitforcompletion&amp;quot;，这是为了等待产品安装过程全部完成：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;FROM ora11g:prepared
MAINTAINER zhangjoto@gmail.com
COPY database /mnt/database
COPY db.rsp /mnt/
COPY sand.dbc /mnt/
RUN sysctl -p &amp;amp;&amp;amp; mount -o remount /dev/shm
RUN chown -R oracle:oinstall /mnt &amp;amp;&amp;amp; chown -R oracle:oinstall /home/oracle
RUN su - oracle -c &amp;quot;/mnt/database/runInstaller -silent -responseFile /mnt/db.rsp -ignorePrereq -waitforcompletion&amp;quot;
RUN /home/oracle/oraInventory/orainstRoot.sh \
    &amp;amp;&amp;amp; /home/oracle/server/product/11.2/db/root.sh
USER oracle
RUN . ~/.bash_profile \
    &amp;amp;&amp;amp; dbca -silent -createDatabase -templateName /mnt/sand.dbc \
        -gdbname sand -sid sand -sysPassword sys -systemPassword sys \
        -characterSet ZHS16GBK -nationalCharacterSet AL16UTF16

CMD /bin/bash
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;但我要很遗憾的说，把这个Dockerfile应用于我前面指定的软件版本的话是要失败的。问题出在这一行代码上：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;RUN sysctl -p &amp;amp;&amp;amp; mount -o remount /dev/shm
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;还记得我前面说过的话么？执行这两个命令必须用--privileged参数为容器指定特权，而Dockerfile现在是不支持使用特权的。有些用户在Docker项目主页提交了issue，希望提供RUNP命令或让build支持--privileged参数，但不知官方最终会作何反应。&lt;a class="footnote-reference" href="#id23" id="id15"&gt;[7]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;如果将来官方对此需求提供了支持，我会回头来修改这一部分的内容。但在此之前，你只能用这个Dockerfile来安装Oracle 11.2.0.1及更低版本的数据库环境，并且记得要删除报错的那一行代码。&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="image"&gt;
&lt;h3&gt;&lt;a class="toc-backref" href="#id37"&gt;环境清理并创建Image&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;至此可用于个人使用的Oracle实验环境就搭建完成了。不过为了更适合于个人使用和作为镜像分发，我决定关闭数据库的审计并清理各种临时文件之后再生成镜像。&lt;/p&gt;
&lt;p&gt;下面的过程我不会做太详细的说明，需要研究的朋友应该很容易就能找到相关的详细资料。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="go"&gt;myDocker$ sqlplus &amp;#39;/as sysdba&amp;#39;&lt;/span&gt;
&lt;span class="go"&gt;SQL&amp;gt; alter user scott account unlock;&lt;/span&gt;

&lt;span class="go"&gt;User altered.&lt;/span&gt;

&lt;span class="go"&gt;SQL&amp;gt; alter system set audit_trail=none scope=spfile;&lt;/span&gt;

&lt;span class="go"&gt;System altered.&lt;/span&gt;

&lt;span class="go"&gt;SQL&amp;gt; shutdown immediate;&lt;/span&gt;
&lt;span class="go"&gt;Database closed.&lt;/span&gt;
&lt;span class="go"&gt;Database dismounted.&lt;/span&gt;
&lt;span class="go"&gt;ORACLE instance shut down.&lt;/span&gt;
&lt;span class="go"&gt;SQL&amp;gt; startup;&lt;/span&gt;
&lt;span class="go"&gt;ORACLE instance started.&lt;/span&gt;

&lt;span class="go"&gt;Total System Global Area  221294592 bytes&lt;/span&gt;
&lt;span class="go"&gt;Fixed Size                  2251936 bytes&lt;/span&gt;
&lt;span class="go"&gt;Variable Size             125829984 bytes&lt;/span&gt;
&lt;span class="go"&gt;Database Buffers           88080384 bytes&lt;/span&gt;
&lt;span class="go"&gt;Redo Buffers                5132288 bytes&lt;/span&gt;
&lt;span class="go"&gt;Database mounted.&lt;/span&gt;
&lt;span class="go"&gt;Database opened.&lt;/span&gt;
&lt;span class="go"&gt;SQL&amp;gt; truncate table SYS.AUD$;&lt;/span&gt;

&lt;span class="go"&gt;Table truncated.&lt;/span&gt;

&lt;span class="go"&gt;SQL&amp;gt; shutdown immediate;&lt;/span&gt;
&lt;span class="go"&gt;SQL&amp;gt; exit&lt;/span&gt;
&lt;span class="go"&gt;myDocker$ rm $ORACLE_BASE/admin/$ORACLE_SID/adump/*&lt;/span&gt;
&lt;span class="go"&gt;myDocker$ rm -rf $ORACLE_HOME/log/`hostname`&lt;/span&gt;
&lt;span class="go"&gt;myDocker$ rm -rf $ORACLE_HOME/diag/tnslsnr/`hostname`&lt;/span&gt;
&lt;span class="go"&gt;myDocker$ rm -rf /tmp/*&lt;/span&gt;
&lt;span class="go"&gt;myDocker$ exit&lt;/span&gt;
&lt;span class="go"&gt;myDocker# yum clean all&lt;/span&gt;
&lt;span class="go"&gt;myDocker# exit&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;现在可以生成一个比较干净的镜像了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gp"&gt;$&lt;/span&gt; docker ps -a
&lt;span class="go"&gt;CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                          PORTS               NAMES&lt;/span&gt;
&lt;span class="go"&gt;375f693461a7        centos:centos6      &amp;quot;/bin/bash&amp;quot;         About an hour ago   Exited (0) About a minute ago                       ora&lt;/span&gt;
&lt;span class="gp"&gt;$&lt;/span&gt; docker commit -m &lt;span class="s2"&gt;&amp;quot;oracle 11.2.0.4 on centos6&amp;quot;&lt;/span&gt; ora ora11g:11.2.0.4
&lt;span class="go"&gt;0de7ebf0d1998e9acfdd9c1bf0841613ce0345b0f430a99b5b4b41f6df837f08&lt;/span&gt;
&lt;span class="gp"&gt;$&lt;/span&gt; docker images
&lt;span class="go"&gt;REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE&lt;/span&gt;
&lt;span class="go"&gt;ora11g              11.2.0.4            0de7ebf0d199        15 minutes ago      6.576 GB&lt;/span&gt;
&lt;span class="go"&gt;centos              centos6             68edf809afe7        3 weeks ago         212.7 MB&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;现在，我们终于有了一套容器中的Oracle数据库实验环境。从此我不必：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;忍受虚拟机的资源消耗；&lt;/li&gt;
&lt;li&gt;等待一大堆系统服务的启动与关闭；&lt;/li&gt;
&lt;li&gt;每做一个试验前都要想清楚怎么将环境恢复原状；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我可以：&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;随时开启实验环境检验自己的想法；&lt;/li&gt;
&lt;li&gt;频繁创建新的实验环境验证不同的技术方案再销毁；&lt;/li&gt;
&lt;li&gt;放心进行各种破坏性测试而不用担心恢复问题；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相比kvm虚拟机，Docker某些场合会给用户带来更多的自由。&lt;/p&gt;
&lt;p&gt;在这次实验过程中，我对于Docker技术有一些新的理解、感想和希望，另外我们还可以对镜像实施一些优化，使其更易于使用。不过这篇文章写到这里已经太长，我想剩下这些内容还是放在后面的文章里说好了。&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id16" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://dgc.uchicago.edu/20130530/mounting-a-kvm-disk-image-without-kvm/"&gt;Mounting a KVM disk image without KVM&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id17" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id8"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="https://docs.docker.com/installation/fedora/#http-proxy"&gt;官方文档：HTTP Proxy&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id18" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id9"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://www.dockerpool.com/article/1413082493"&gt;DockerPool FAQ&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id19" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id10"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://www.dockerpool.com/static/books/docker_practice/image/create.html"&gt;创建镜像|Docker----从入门到实践&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id21" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id12"&gt;[5]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://blog.docker.com/2014/06/why-you-dont-need-to-run-sshd-in-docker/"&gt;Why you don't need to run SSHd in your Docker containers&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id22" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id13"&gt;[6]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://fabiorehm.com/blog/2014/09/11/running-gui-apps-with-docker/"&gt;Running GUI apps with Docker&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id23" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id15"&gt;[7]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="https://github.com/docker/docker/issues/1916"&gt;docker build should support privileged operations&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</summary><category term="Docker"></category><category term="Oracle"></category><category term="容器技术"></category></entry></feed>